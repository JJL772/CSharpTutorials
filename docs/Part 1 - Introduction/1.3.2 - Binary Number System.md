#Binary Number System
The binary number system, also known as the base-2 number system, is a number system initially developed and used all the way back in the Egyptian times. Today, the binary number system is scarcely used outside of the computer sciences, but inside the computer sciences, the binary number system is absolutely vital. The binary number system uses only two values, a 1 or a 0 to represent numbers, as hinted by the bi- in the name. Other than the different digits, counting in binary is nearly identical to counting in our decimal (base-10) number system.

#Why do we use binary?
The binary number system is used in the computer sciences because computers can only represent 0 or 1 reliably in memory and in the processor. Why? Computer memory uses switches which only have 2 states they can be in, on or off. This isn't always true, however, quantum computers for instance, use qbits, which can store any number of possible values. Until we can buy quantum computers, we won't be using anything but binary. Not only that, but binary is incredibly easy to understand for certain tasks, which we will get into later. Trust me, you'll be glad for binary.

#Counting in Binary
Counting in binary is very similar to counting in decimal. Here's a table of decimal numbers 0-20 mapped to their binary counterparts.

| Binary | Decimal |
|:------:|:-------:|
| 0 | 0 |
| 1 | 1 |
| 10 | 2 |
| 11 | 3 |
| 100 | 4 |
| 101 | 5 |
| 110 | 6 |
| 111 | 7 |
| 1000 | 8 |
| 1001 | 9 |
| 1010 | 10 |
| 1011 | 11 |
| 1100 | 12 |
| 1101 | 13 |
| 1110 | 14 |
| 1111 | 15 |
| 10000 | 16 |
| 10001 | 17 |
| 10010 | 18 |
| 10011 | 19 |
| 10100 | 20 |

Hopefully you can see a pattern in this table, if not, here's how the pattern goes. For this, we will call each place in the number string a bit, as it should be called. If we start at `0` and increment the value, we will get `1`, if we increment again we will get `10`. Why? When we fill up the least significant bit's (the furthest right bit) value we will then increment the next most significant bit's (the bit to the left of this one) value. So when we count to `1`, we can't add any more numbers to this bit's place, so we will then increment the next most significant bit's value, resulting in the value `10` or `2` in decimal. If we want to count up again, we will perform a similar operation. We will then try to increment the least significant bit's value, giving us the result `11` or `3` in decimal. But what's next? Simple, we repeat the operation, we will try to increment the least significant bit's value, but that fails, so we'll set it to `0`, then move onto the next bit and try to increment it. That too will fail, so we set the value of that bit to `0`. We will now move onto the next position, which is the furthest right. When we try to increment this value, it will succeed, and we have completed our operation, resulting in a binary value of `100` or `4` in decimal.

Here is a step by step procedure to count in binary:
Note: Failing to increment a value is defined as trying to increase that bit's value when that bit us already equal `1`, so that it can't equal anything greater. In other words, try to add 1 onto the bit's value, but if it results in a value greater than `1` it's a failure.

| Steps |
|:-----:|
| Try to increment the least significant bit's value |
| If that fails, set this bit's value to `0` and try to increment the next most significant bit's value |
| Continue until you can increment a value successfully |

#Endianness
Throughout this document, you've heard me use the terms "most significant bit" and "least significant bit." Hastily, I defined them as the first and last bits in a binary number, respectively. However, as always, programming is filled with arguments which really should not exist. One of these arguments is the argument of endianness. Endianness refers to the sequential order in which bits are numbered. Yes, I know what you're thinking, something as simple as that should not be the basis for an argument, but it is, and we have to deal with it. Throughout this tutorial we will be using the big-endian format, meaning the bits in a binary number are numbered from the left to the right. On a little-endian system, the bits are numbered in the opposite way, decreasing from the left to the right.

The reason why understanding endianess is so important, is because a mixup between the endianness formats will completely change the meaning of the number. For example, lets say I have the number `2` in binary, which is `10` in a big-endian format. Now lets say I switch the endianness of that number to little-endian, assuming we aren't going to move any bits around, we would still have `10`, but in a little-endian format. No problem right? Wrong! When we try to convert our little endian number back to decimal, using our new endian format, that would result in the number `1` in decimal. But why? WHAT IS THIS BLACK MAGIC!
Here's a little visualization of what exactly endianness means. (Be sure to read the table carefully)
Note: Bits are numbered starting from 0, because it represents an offset rather than an index.
Another Note: The number we will be using will be `14` in decimal.

| Big-Endian Numbering | Binary Number | Little-Endian Numbering |
|:--------------------:|:-------------:|:-----------------------:|
| 0 | 1 | 3 |
| 1 | 1 | 2 |
| 2 | 1 | 1 |
| 3 | 0 | 0 |

So what exactly does this table mean? Here's an interpretation, compressed down quite a bit.
In Big-Endian format, the number `14` is `1110`
In Little-Endian format, the number `14` is `0111`
As you can tell they are flipped, as the numbering of the bits is flipped around.

I strongly encourage you to not form an opinion about "which endianness is surperior", as that's how bad programmers are created. Use what is necessary when it is necessary, and adhere to standards to prevent confusion.
